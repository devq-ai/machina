[tool:pytest]
# PyTest Configuration for SurrealDB MCP Server Tests
# Following DevQ.ai MCP testing standards with 100% success rate requirements

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test execution
addopts =
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov=surrealdb_mcp
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=90
    --durations=10
    --maxfail=5
    --asyncio-mode=auto

# Test markers
markers =
    core_functionality: Core MCP functionality tests
    integration: Integration tests with real SurrealDB service
    performance: Performance and benchmarking tests
    error_handling: Error handling and recovery tests
    compliance: MCP protocol compliance tests
    slow: Slow running tests (>10s)
    real_service: Tests requiring real SurrealDB connection
    mock_service: Tests using mock SurrealDB
    unit: Unit tests
    end_to_end: End-to-end workflow tests

# Test filtering
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:websockets.*
    ignore::ResourceWarning

# Asyncio configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Coverage configuration
[coverage:run]
source = surrealdb_mcp
omit =
    */tests/*
    */venv/*
    */env/*
    */__pycache__/*
    */build/*
    */dist/*
    setup.py
    conftest.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

# Performance test configuration
performance_timeout = 1.0

# Test success criteria
minimum_success_rate = 100.0
maximum_failures = 0
minimum_coverage = 90.0
