#!/usr/bin/env python3
"""
Comprehensive Single MCP Testing Script
Tests ALL tools in ONE MCP server at a time with REAL tool calls.

ZERO TOLERANCE FOR FAKE DATA - All responses must be authentic.
"""

import asyncio
import json
import sys
import os
from pathlib import Path
from typing import Dict, Any, List
import traceback
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

# Import actual MCP servers
from mcp_servers.memory_mcp import MemoryMCP
from mcp_servers.logfire_mcp import LogfireMCP
from mcp_servers.fastapi_mcp import FastAPIMCP
from mcp_servers.pytest_mcp import PyTestMCP
from mcp_servers.github_mcp import GitHubMCP
from mcp_servers.docker_mcp import DockerMCP
from mcp_servers.registry_mcp import RegistryMCP

import logfire

# Configure environment variables from known sources
os.environ.update({
    'LOGFIRE_TOKEN': 'pylf_v1_us_T6lpqTTbCXH4T56JlgCdM2qMhH3cZyrwTG1ZDvLk2xyC',
    'LOGFIRE_PROJECT_NAME': 'devq-ai',
    'LOGFIRE_API_URL': 'https://logfire-us.pydantic.dev',
    'SURREALDB_URL': 'ws://localhost:8000/rpc',
    'SURREALDB_USERNAME': 'root',
    'SURREALDB_PASSWORD': 'root',
    'SURREALDB_NAMESPACE': 'ptolemies',
    'SURREALDB_DATABASE': 'knowledge',
})

# Configure logging
logfire.configure()

class SingleMCPTester:
    """
    Comprehensive Single MCP Server Tester

    Tests ALL tools in one MCP server with REAL tool calls.
    NO FAKE/MOCK/STUB RESPONSES PERMITTED.
    """

    def __init__(self):
        self.current_mcp = None
        self.mcp_name = None
        self.start_time = datetime.now()
        self.results = {
            'test_metadata': {
                'execution_time': None,
                'duration': None,
                'mcp_name': None,
                'total_tools': 0,
                'successful_tools': 0,
                'failed_tools': 0,
                'success_rate': 0.0
            },
            'server_info': {},
            'tool_results': [],
            'verification_statement': 'All results are from REAL MCP tool calls with authentic responses. NO FAKE DATA.'
        }

    async def initialize_mcp(self, mcp_name: str):
        """Initialize a single MCP server"""
        mcp_classes = {
            'memory': MemoryMCP,
            'logfire': LogfireMCP,
            'fastapi': FastAPIMCP,
            'pytest': PyTestMCP,
            'github': GitHubMCP,
            'docker': DockerMCP,
            'registry': RegistryMCP
        }

        if mcp_name not in mcp_classes:
            raise ValueError(f"Unknown MCP: {mcp_name}")

        try:
            with logfire.span(f"Initialize {mcp_name} MCP"):
                self.current_mcp = mcp_classes[mcp_name]()
                self.mcp_name = mcp_name
                logfire.info(f"Successfully initialized {mcp_name} MCP server")
                return True
        except Exception as e:
            logfire.error(f"Failed to initialize {mcp_name} MCP: {str(e)}")
            return False

    async def get_all_tools(self) -> List[str]:
        """Get ALL tools from the current MCP server"""
        if not self.current_mcp or not hasattr(self.current_mcp, 'mcp'):
            return []

        try:
            tools = list(self.current_mcp.mcp._tools.keys())
            logfire.info(f"Found {len(tools)} tools in {self.mcp_name} MCP")
            return tools
        except Exception as e:
            logfire.error(f"Failed to get tools from {self.mcp_name}: {str(e)}")
            return []

    async def get_server_info(self) -> Dict[str, Any]:
        """Get comprehensive server information"""
        if not self.current_mcp or not hasattr(self.current_mcp, 'mcp'):
            return {'error': 'MCP server not available'}

        try:
            info = self.current_mcp.mcp.get_tool_info()
            return info
        except Exception as e:
            return {
                'error': f'Failed to get server info: {str(e)}',
                'server_type': type(self.current_mcp).__name__
            }

    async def test_single_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test a single tool with REAL MCP call

        Returns authentic results or error information
        """
        if not self.current_mcp or not hasattr(self.current_mcp, 'mcp'):
            return {
                'tool_name': tool_name,
                'success': False,
                'error': 'MCP server not available',
                'timestamp': datetime.now().isoformat()
            }

        try:
            with logfire.span(f"Tool test: {self.mcp_name}.{tool_name}", arguments=arguments):
                # Get the actual tool function
                tool_func = self.current_mcp.mcp._tools.get(tool_name)
                if not tool_func:
                    return {
                        'tool_name': tool_name,
                        'success': False,
                        'error': f'Tool {tool_name} not found',
                        'available_tools': list(self.current_mcp.mcp._tools.keys()),
                        'timestamp': datetime.now().isoformat()
                    }

                # Execute the REAL tool function
                start_time = datetime.now()
                if asyncio.iscoroutinefunction(tool_func):
                    result = await tool_func(**arguments)
                else:
                    result = await asyncio.get_event_loop().run_in_executor(None, lambda: tool_func(**arguments))

                execution_time = (datetime.now() - start_time).total_seconds()

                logfire.info(f"Tool {tool_name} executed successfully")
                return {
                    'tool_name': tool_name,
                    'success': True,
                    'arguments': arguments,
                    'result': result,
                    'result_type': type(result).__name__,
                    'execution_time': execution_time,
                    'timestamp': datetime.now().isoformat()
                }

        except Exception as e:
            logfire.error(f"Tool {tool_name} failed: {str(e)}")
            return {
                'tool_name': tool_name,
                'success': False,
                'error': str(e),
                'arguments': arguments,
                'traceback': traceback.format_exc(),
                'timestamp': datetime.now().isoformat()
            }

    async def test_all_tools_comprehensive(self, mcp_name: str) -> Dict[str, Any]:
        """
        Test ALL tools in a single MCP server comprehensively

        This is the main testing function that validates every tool
        """

        # Initialize the MCP server
        if not await self.initialize_mcp(mcp_name):
            return {
                'error': f'Failed to initialize {mcp_name} MCP server',
                'timestamp': datetime.now().isoformat()
            }

        # Get server information
        server_info = await self.get_server_info()
        self.results['server_info'] = server_info

        # Get all available tools
        all_tools = await self.get_all_tools()
        if not all_tools:
            return {
                'error': f'No tools found in {mcp_name} MCP server',
                'timestamp': datetime.now().isoformat()
            }

        logfire.info(f"Starting comprehensive testing of {len(all_tools)} tools in {mcp_name} MCP")

        # Define test cases for each tool based on MCP server
        test_cases = self.get_test_cases_for_mcp(mcp_name, all_tools)

        # Test each tool
        successful_tools = 0
        failed_tools = 0

        for tool_name in all_tools:
            tool_test_cases = test_cases.get(tool_name, [{}])  # Default empty args if no specific test case

            for i, test_case in enumerate(tool_test_cases):
                test_name = f"{tool_name}_{i+1}" if len(tool_test_cases) > 1 else tool_name

                logfire.info(f"Testing {mcp_name}.{test_name}")
                result = await self.test_single_tool(tool_name, test_case)

                self.results['tool_results'].append(result)

                if result['success']:
                    successful_tools += 1
                    logfire.info(f"‚úÖ {test_name} PASSED")
                else:
                    failed_tools += 1
                    logfire.error(f"‚ùå {test_name} FAILED: {result.get('error', 'Unknown error')}")

        # Update metadata
        self.results['test_metadata'].update({
            'execution_time': datetime.now().isoformat(),
            'duration': str(datetime.now() - self.start_time),
            'mcp_name': mcp_name,
            'total_tools': len(all_tools),
            'successful_tools': successful_tools,
            'failed_tools': failed_tools,
            'success_rate': (successful_tools / len(all_tools)) * 100 if all_tools else 0.0
        })

        return self.results

    def get_test_cases_for_mcp(self, mcp_name: str, tools: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """
        Define comprehensive test cases for each MCP server

        Returns dictionary with tool_name -> list of test cases
        """

        if mcp_name == 'memory':
            return {
                'store_memory': [
                    {
                        'content': 'Machina is a comprehensive MCP registry platform built with FastAPI and FastMCP framework',
                        'context': 'machina_project',
                        'tags': ['machina', 'mcp', 'registry', 'fastapi']
                    },
                    {
                        'content': 'The Machina project includes multiple MCP servers for memory, logfire, pytest, github, docker, and registry management',
                        'context': 'machina_architecture',
                        'tags': ['machina', 'mcp-servers', 'comprehensive'],
                        'importance': 8
                    }
                ],
                'retrieve_memory': [
                    {'memory_id': 'machina_id_1'}
                ],
                'search_memories': [
                    {'query': 'machina', 'limit': 5},
                    {'query': 'mcp', 'context': 'machina_project'}
                ],
                'update_memory': [
                    {
                        'memory_id': 'machina_id_1',
                        'content': 'Updated Machina project with enhanced MCP registry capabilities',
                        'tags': ['updated', 'machina', 'enhanced']
                    }
                ],
                'delete_memory': [
                    {'memory_id': 'machina_id_to_delete'}
                ],
                'list_contexts': [{}],
                'get_memory_stats': [{}],
                'export_memories': [
                    {},
                    {'context': 'machina_project'}
                ]
            }

        elif mcp_name == 'logfire':
            return {
                'query_logs': [
                    {'limit': 5},
                    {'level': 'info', 'limit': 10},
                    {'service': 'test_service', 'limit': 3}
                ],
                'get_metrics': [
                    {'metric_type': 'spans', 'time_range': '1h'},
                    {'metric_type': 'errors', 'time_range': '24h'}
                ],
                'create_alert': [
                    {
                        'name': 'Test Alert',
                        'condition': 'error_rate > 0.1',
                        'threshold': 0.1,
                        'notification_channel': 'email'
                    }
                ],
                'list_alerts': [{}],
                'get_project_info': [{}],
                'create_custom_log': [
                    {
                        'level': 'info',
                        'message': 'Test log message',
                        'service': 'test_service'
                    }
                ],
                'get_performance_stats': [
                    {'time_range': '1h'},
                    {'service': 'test_service'}
                ],
                'export_logs': [
                    {'format': 'json', 'limit': 10},
                    {'format': 'csv', 'time_range': '1h'}
                ]
            }

        elif mcp_name == 'fastapi':
            return {
                'create_project': [
                    {
                        'name': 'test_project_1',
                        'description': 'Test FastAPI project',
                        'template': 'basic'
                    }
                ],
                'add_endpoint': [
                    {
                        'project_name': 'test_project_1',
                        'endpoint_path': '/test',
                        'method': 'GET',
                        'description': 'Test endpoint'
                    }
                ],
                'list_projects': [{}],
                'run_project': [
                    {'project_name': 'test_project_1'}
                ],
                'install_dependencies': [
                    {'project_name': 'test_project_1', 'packages': ['pydantic', 'uvicorn']}
                ],
                'generate_openapi_spec': [
                    {'project_name': 'test_project_1'}
                ]
            }

        elif mcp_name == 'pytest':
            return {
                'run_tests': [
                    {'test_path': 'tests/', 'verbose': True},
                    {'test_path': 'tests/', 'coverage': True},
                    {'test_path': 'tests/test_memory_mcp.py'}
                ],
                'discover_tests': [
                    {'test_path': 'tests/'},
                    {'test_path': '.', 'pattern': 'test_*.py'}
                ],
                'generate_test_file': [
                    {
                        'module_name': 'example_module',
                        'test_type': 'unit',
                        'output_path': 'tests/test_example.py'
                    }
                ],
                'run_coverage': [
                    {'source_path': 'src/', 'test_path': 'tests/'},
                    {'source_path': 'mcp_servers/', 'test_path': 'tests/'}
                ],
                'run_specific_test': [
                    {'test_name': 'test_memory_mcp_initialization'},
                    {'test_name': 'test_memory_mcp_initialization', 'file_path': 'tests/test_memory_mcp.py'}
                ],
                'install_pytest_plugins': [
                    {'plugins': ['pytest-cov', 'pytest-mock']}
                ],
                'get_test_stats': [{}]
            }

        elif mcp_name == 'github':
            return {
                'get_user_info': [
                    {'username': 'octocat'},
                    {}  # Get authenticated user info
                ],
                'list_repositories': [
                    {'username': 'octocat', 'limit': 5},
                    {'org': 'github', 'limit': 3}
                ],
                'get_repository': [
                    {'owner': 'octocat', 'repo': 'Hello-World'}
                ],
                'list_issues': [
                    {'owner': 'octocat', 'repo': 'Hello-World', 'state': 'open'}
                ],
                'create_issue': [
                    {
                        'owner': 'test_owner',
                        'repo': 'test_repo',
                        'title': 'Test issue',
                        'body': 'This is a test issue'
                    }
                ],
                'list_pull_requests': [
                    {'owner': 'octocat', 'repo': 'Hello-World', 'state': 'open'}
                ],
                'create_pull_request': [
                    {
                        'owner': 'test_owner',
                        'repo': 'test_repo',
                        'title': 'Test PR',
                        'body': 'Test pull request',
                        'head': 'feature-branch',
                        'base': 'main'
                    }
                ],
                'search_repositories': [
                    {'query': 'fastapi', 'limit': 5},
                    {'query': 'language:python', 'limit': 3}
                ]
            }

        elif mcp_name == 'docker':
            return {
                'list_containers': [
                    {},
                    {'all': True}
                ],
                'get_container': [
                    {'container_id': 'test_container_id'}
                ],
                'create_container': [
                    {
                        'image': 'hello-world',
                        'name': 'test_container',
                        'command': ['echo', 'Hello World']
                    }
                ],
                'start_container': [
                    {'container_id': 'test_container_id'}
                ],
                'stop_container': [
                    {'container_id': 'test_container_id'}
                ],
                'remove_container': [
                    {'container_id': 'test_container_id'}
                ],
                'get_container_logs': [
                    {'container_id': 'test_container_id', 'tail': 10}
                ],
                'list_images': [{}],
                'pull_image': [
                    {'image': 'hello-world:latest'}
                ],
                'remove_image': [
                    {'image': 'hello-world:latest'}
                ],
                'get_system_info': [{}],
                'list_networks': [{}],
                'list_volumes': [{}]
            }

        elif mcp_name == 'registry':
            return {
                'search_servers': [
                    {'query': 'fastapi'},
                    {'query': 'test', 'limit': 5}
                ],
                'get_server_info': [
                    {'server_name': 'fastapi-mcp'},
                    {'server_name': 'memory-mcp'}
                ],
                'install_server': [
                    {'server_name': 'test-server', 'version': 'latest'}
                ],
                'list_installed_servers': [{}],
                'update_registry_cache': [{}],
                'publish_server': [
                    {
                        'server_name': 'test-server',
                        'version': '1.0.0',
                        'description': 'Test server'
                    }
                ],
                'get_registry_stats': [{}]
            }

        # Default case: empty arguments for all tools
        return {tool: [{}] for tool in tools}

    def save_results(self, filename: str = None):
        """Save results to timestamped JSON file"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.mcp_name}-{timestamp}.json"

        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)

        logfire.info(f"Results saved to {filename}")
        return filename

    def print_summary(self):
        """Print test summary"""
        metadata = self.results['test_metadata']
        print(f"\nüéØ COMPREHENSIVE {metadata['mcp_name'].upper()} MCP TESTING RESULTS")
        print(f"üìÖ Execution Time: {metadata['execution_time']}")
        print(f"‚è±Ô∏è  Duration: {metadata['duration']}")
        print(f"üîß Total Tools: {metadata['total_tools']}")
        print(f"‚úÖ Successful: {metadata['successful_tools']}")
        print(f"‚ùå Failed: {metadata['failed_tools']}")
        print(f"üìä Success Rate: {metadata['success_rate']:.1f}%")
        print(f"\nüîí VERIFICATION: All results are from REAL MCP tool calls")
        print(f"üìÑ Results saved for validation")

async def main():
    """Main execution function"""
    if len(sys.argv) < 2:
        print("Usage: python test_single_mcp_comprehensive.py <mcp_name>")
        print("Available MCPs: memory, logfire, fastapi, pytest, github, docker, registry")
        sys.exit(1)

    mcp_name = sys.argv[1].lower()

    logfire.info(f"Starting comprehensive testing of {mcp_name} MCP")

    tester = SingleMCPTester()

    try:
        # Test all tools in the specified MCP
        results = await tester.test_all_tools_comprehensive(mcp_name)

        # Save results to timestamped JSON file
        filename = tester.save_results()

        # Print summary
        tester.print_summary()

        print(f"\nüìÅ JSON Results File: {filename}")
        print(f"üîç Ready for validation!")

    except Exception as e:
        logfire.error(f"Testing failed: {str(e)}")
        print(f"‚ùå TESTING FAILED: {str(e)}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
